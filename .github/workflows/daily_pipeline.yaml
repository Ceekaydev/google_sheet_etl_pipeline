name: Daily Data Pipeline

on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - 'scripts__/**'
      - '.github/workflows/**'

jobs:
  process-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      # Added 'openpyxl' just in case pandas needs it for Excel export later
      run: |
        pip install pandas pyarrow gspread oauth2client duckdb dotenv openpyxl

    - name: Create credentials file
      run: |
        mkdir -p secret
        cat << 'EOF' > secret/sheets-pipeline-483414-4c2de1d1423c.json
        ${{ secrets.GOOGLE_SHEETS_CREDS_FILE }}
        EOF

    - name: Run data pipeline
      env:
        GOOGLE_SHEETS_CREDS_FILE: ${{ github.workspace }}/secret/sheets-pipeline-483414-4c2de1d1423c.json
        SOURCE_SHEET_URL: ${{ secrets.SOURCE_SHEET_URL }}
        OUTPUT_SHEET_URL: ${{ secrets.OUTPUT_SHEET_URL }}
      run: |
        cd scripts__
        python main.py
    
    - name: Upload parquet artifacts
      uses: actions/upload-artifact@v4
      with:
        name: monthly-data
        # CRITICAL FIX: Point to the data folder INSIDE scripts__
        # checks subdirectories (raw, processed, aggregated)
        path: scripts__/data/**/*.parquet 
        retention-days: 90
        if-no-files-found: ignore